{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an implementation of the iLPD method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim\n",
    "import tensorboardX\n",
    "import memcnn\n",
    "import time\n",
    "\n",
    "import odl\n",
    "from odl.contrib import fom\n",
    "from odl.contrib import torch as odl_torch\n",
    "\n",
    "from generator_downsampled_120ang import DataLoader\n",
    "from utils import *\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "name = 'invertible_walnuts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader()\n",
    "\n",
    "# the first walnut is used for validation, ind=1\n",
    "# the second for testing, rest is for training\n",
    "x_val, y_val, geometry_val = loader.generate_data(mode=\"val\")\n",
    "val_images = torch.from_numpy(x_val).type(dtype)\n",
    "val_data = torch.from_numpy(y_val).type(dtype)\n",
    "\n",
    "# define the Ray Transform operator\n",
    "space_val = loader.odl_space()\n",
    "operator_val = odl.tomo.RayTransform(space_val, geometry_val, impl='astra_cuda') \n",
    "\n",
    "# transform the operator and it's adjoint into pytorch modules\n",
    "pt_op_val = odl_torch.OperatorModule(operator_val).type(dtype)\n",
    "pt_op_adj_val = odl_torch.OperatorModule(operator_val.adjoint).type(dtype)\n",
    "\n",
    "# Compute the operator norm. After each application of the operator, \n",
    "# divide the result by it's norm for numerical stability.\n",
    "# Since the norm is almost the same for different walnuts, \n",
    "# it is enough to compute it for once.\n",
    "opnorm = 51.5593061617 # operator_val.norm(estimate=True)\n",
    "print(\"Norm of the operator:\", opnorm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the iLPD architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 1 # batch size\n",
    "n_iter = 20 # number of unrolled iterations\n",
    "# number of primal and dual memory channels\n",
    "n_primal = 1 \n",
    "n_dual = 1\n",
    "n_filters = 32 # in a convolutional layer\n",
    "\n",
    "\n",
    "class IterationPrimal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.filters = n_filters\n",
    "        # The adjoint operator is implemented as a parameter \n",
    "        # for techical reason related to memcnn.\n",
    "        # However, it should be updated as if was an input variable.\n",
    "        self.op_adj = None\n",
    "\n",
    "        self.primalblock = nn.Sequential(\n",
    "            nn.Conv3d(n_primal, self.filters, 3, padding=1),\n",
    "            nn.PReLU(num_parameters=self.filters, init=0.0),\n",
    "            nn.Conv3d(self.filters, self.filters, 3, padding=1),\n",
    "            nn.PReLU(num_parameters=self.filters, init=0.0),\n",
    "            nn.Conv3d(self.filters, n_primal, 3, padding=1))\n",
    "            \n",
    "\n",
    "    def forward(self, primal, dual):\n",
    "        inp = self.op_adj(dual) / opnorm\n",
    "        primal = primal + self.primalblock(inp)\n",
    "        return primal, dual.clone()\n",
    "    \n",
    "    def inverse(self, primal, dual):\n",
    "        inp = self.op_adj(dual) / opnorm\n",
    "        primal = primal - self.primalblock(inp)\n",
    "        return primal, dual.clone()\n",
    "    \n",
    "class IterationDual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.filters = n_filters\n",
    "        # The operator and input data are implemented as parameters \n",
    "        # for techical reason related to memcnn.\n",
    "        # However, they should be updated as if they were input variables.\n",
    "        self.op = None\n",
    "        self.y = None\n",
    "        \n",
    "        self.dualblock = nn.Sequential(\n",
    "            nn.Conv3d(n_dual + 1, self.filters, 3, padding=1),\n",
    "            nn.PReLU(num_parameters=self.filters, init=0.0),\n",
    "            nn.Conv3d(self.filters, self.filters, 3, padding=1),\n",
    "            nn.PReLU(num_parameters=self.filters, init=0.0),\n",
    "            nn.Conv3d(self.filters, n_dual, 3, padding=1))\n",
    "            \n",
    "    def forward(self, primal, dual):\n",
    "        evalop = self.op(primal) / opnorm\n",
    "        inp = torch.cat([evalop, self.y / opnorm ], dim=1)\n",
    "        dual = dual + self.dualblock(inp)\n",
    "        return primal.clone(), dual\n",
    "    \n",
    "    \n",
    "    def inverse(self, primal, dual):\n",
    "        evalop = self.op(primal) / opnorm\n",
    "        inp = torch.cat([evalop, self.y / opnorm ], dim=1)\n",
    "        dual = dual - self.dualblock(inp)\n",
    "        return primal.clone(), dual\n",
    "    \n",
    "class IterativeNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            iteration_p = IterationPrimal()\n",
    "            iteration_d = IterationDual()\n",
    "            inv_iteration_p = memcnn.InvertibleModuleWrapper(fn=iteration_p, keep_input=False, keep_input_inverse=False)\n",
    "            inv_iteration_d = memcnn.InvertibleModuleWrapper(fn=iteration_d, keep_input=False, keep_input_inverse=False)\n",
    "            setattr(self, 'iteration_p_{}'.format(i), inv_iteration_p)\n",
    "            setattr(self, 'iteration_d_{}'.format(i), inv_iteration_d)\n",
    "\n",
    "    def forward(self, y, true, op, op_adj):\n",
    "        im_shape = true.shape[2:]\n",
    "        data_shape = y.shape[2:]\n",
    "        primal = torch.zeros((n_data, n_primal) + im_shape).type(dtype)\n",
    "        dual = torch.zeros((n_data, n_dual) + data_shape).type(dtype)\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            iteration_p = getattr(self, 'iteration_p_{}'.format(i))\n",
    "            iteration_d = getattr(self, 'iteration_d_{}'.format(i))\n",
    "            iteration_d._fn.op = op\n",
    "            iteration_d._fn.y = y\n",
    "            iteration_p._fn.op_adj = op_adj\n",
    "            primal, dual = iteration_d(primal, dual)\n",
    "            primal, dual = iteration_p(primal, dual)\n",
    "        \n",
    "        res = primal[:, 0:1, ...]\n",
    "        loss = self.loss(res, true)\n",
    "            \n",
    "        return res, loss\n",
    "    \n",
    "    def inv_module_eval(self):\n",
    "        for i in range(n_iter):\n",
    "            iteration = getattr(self, 'iteration_p_{}'.format(i))\n",
    "            iteration.num_bwd_passes = 0 # number of backward passes\n",
    "            iteration.eval()\n",
    "            iteration = getattr(self, 'iteration_d_{}'.format(i))\n",
    "            iteration.num_bwd_passes = 0\n",
    "            iteration.eval()\n",
    "            \n",
    "    def inv_module_train(self):\n",
    "        for i in range(n_iter):\n",
    "            iteration = getattr(self, 'iteration_p_{}'.format(i))\n",
    "            iteration.num_bwd_passes = 1 # number of backward passes\n",
    "            iteration.train()\n",
    "            iteration = getattr(self, 'iteration_d_{}'.format(i))\n",
    "            iteration.num_bwd_passes = 1\n",
    "            iteration.train()\n",
    "\n",
    "# This is \"Xavier\" initialization of weights.\n",
    "# It is very important for the training!!!\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find('Conv3d') != -1:\n",
    "        shape = m.weight.shape\n",
    "        lim = np.sqrt(6 / (shape[0] + shape[1]) / shape[2] / shape[3] / shape[4])\n",
    "        m.weight.data.uniform_(-lim, lim)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "iter_net = IterativeNetwork().type(dtype)\n",
    "iter_net.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tensorboardX.SummaryWriter(comment=\"/train_\"+name)\n",
    "val_writer = tensorboardX.SummaryWriter(comment=\"/val_\"+name)\n",
    "\n",
    "maximum_steps = (100000 // 20) + 1\n",
    "starter_learning_rate = 0.5 * 1e-3\n",
    "optimizer = torch.optim.Adam(iter_net.parameters(), lr=starter_learning_rate, betas=(0.9, 0.99))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, maximum_steps)\n",
    "\n",
    "for i in range(maximum_steps):\n",
    "    iter_net.train()\n",
    "    iter_net.inv_module_train()\n",
    "\n",
    "    x, y, geometry = loader.generate_data(mode=\"train\")\n",
    "    images = torch.from_numpy(x).type(dtype)\n",
    "    data = torch.from_numpy(y).type(dtype)\n",
    "\n",
    "    space = loader.odl_space()\n",
    "    operator = odl.tomo.RayTransform(space, geometry, impl='astra_cuda') \n",
    "    pt_op = odl_torch.OperatorModule(operator).type(dtype)\n",
    "    pt_op_adj = odl_torch.OperatorModule(operator.adjoint).type(dtype)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output, loss = iter_net(data, images, pt_op, pt_op_adj)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(iter_net.parameters(), max_norm=1.0, norm_type=2)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        iter_net.eval()\n",
    "        iter_net.inv_module_eval()\n",
    "        output_val, loss_val = iter_net(val_data, val_images, pt_op_val, pt_op_adj_val)\n",
    "        summaries(val_writer, output_val, val_images, loss_val, i, do_print=True)\n",
    "\n",
    "    if i > 0 and i % 100 == 0:\n",
    "        torch.save(iter_net.state_dict(), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_net.load_state_dict(torch.load(name))\n",
    "\n",
    "x_test, y_test, geometry_test = loader.generate_data(mode=\"test\")\n",
    "test_images = torch.from_numpy(x_test).type(dtype)\n",
    "test_data = torch.from_numpy(y_test).type(dtype)\n",
    "\n",
    "space_test = loader.odl_space()\n",
    "operator_test = odl.tomo.RayTransform(space_test, geometry_test, impl='astra_cuda') \n",
    "\n",
    "pt_op_test = odl_torch.OperatorModule(operator_test).type(dtype)\n",
    "pt_op_adj_test = odl_torch.OperatorModule(operator_test.adjoint).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_net.eval()\n",
    "iter_net.inv_module_eval()\n",
    "\n",
    "start = time.time()\n",
    "output_test, loss_test = iter_net(test_data, test_images, pt_op_test, pt_op_adj_test)\n",
    "rec = output_test.detach().cpu().numpy()[0,0]\n",
    "print(\"Time: \", time.time() - start)\n",
    "\n",
    "show(rec)\n",
    "print(\"MSE: \", fom.mean_squared_error(rec, x_test[0,0]))\n",
    "print(\"PSNR: \", fom.psnr(rec, x_test[0,0]))\n",
    "print(\"SSIM: \", fom.ssim(rec, x_test[0,0]))\n",
    "plot3D(rec, \"iLPD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTORCH",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
